{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import BounceState\n",
    "from alpha_connect import *\n",
    "from random import choice\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "GameChoice.set_game(BounceState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldmodel = AlphaZeroModelBounce()\n",
    "oldmodel.load_state_dict(torch.load(\"../data/early_bounce.pth\"))\n",
    "old_neural_net_agent = NeuralNetAgent(oldmodel)\n",
    "model = AlphaZeroModelBounce()\n",
    "model.load_state_dict(torch.load(\"../data/latest_bounce.pth\"))\n",
    "neural_net_agent = NeuralNetAgent(model)\n",
    "agents = [MCTSAgent(5000), OneMoveAheadAgent()]\n",
    "# random : mean: 37.813, max: 283, min: 4\n",
    "# selective/selective : mean: 20.0, max: 116, min: 7\n",
    "# selective/random : mean: 13.218, max: 71, min: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0\n",
      "{'agent1': 0, 'agent2': 0, 'draws': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1\n",
      "{'agent1': 0, 'agent2': 1, 'draws': 0}\n",
      "Game 2\n",
      "{'agent1': 1, 'agent2': 1, 'draws': 0}\n",
      "Game 3\n",
      "{'agent1': 2, 'agent2': 1, 'draws': 0}\n",
      "Game 4\n",
      "{'agent1': 3, 'agent2': 1, 'draws': 0}\n",
      "Game 5\n",
      "{'agent1': 4, 'agent2': 1, 'draws': 0}\n",
      "Game 6\n",
      "{'agent1': 5, 'agent2': 1, 'draws': 0}\n",
      "Game 7\n",
      "{'agent1': 6, 'agent2': 1, 'draws': 0}\n",
      "Game 8\n",
      "{'agent1': 6, 'agent2': 2, 'draws': 0}\n",
      "Game 9\n",
      "{'agent1': 7, 'agent2': 2, 'draws': 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplay_2players_games\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMCTSAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOneMoveAheadAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/helper.py:36\u001b[0m, in \u001b[0;36mplay_2players_games\u001b[0;34m(agent1, agent2, n, random_start_agent, greedy)\u001b[0m\n\u001b[1;32m     34\u001b[0m         result \u001b[38;5;241m=\u001b[39m play_game(agents, greedy\u001b[38;5;241m=\u001b[39mgreedy)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mresult\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/helper.py:19\u001b[0m, in \u001b[0;36mplay_game\u001b[0;34m(agents, starting_state, greedy)\u001b[0m\n\u001b[1;32m     17\u001b[0m gamestate \u001b[38;5;241m=\u001b[39m Game(agents, starting_state)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gamestate\u001b[38;5;241m.\u001b[39mhas_ended():\n\u001b[0;32m---> 19\u001b[0m     gamestate \u001b[38;5;241m=\u001b[39m \u001b[43mgamestate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#show_board(gamestate.state)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gamestate\u001b[38;5;241m.\u001b[39mreward()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/game.py:21\u001b[0m, in \u001b[0;36mGame.play\u001b[0;34m(self, greedy)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay\u001b[39m(\u001b[38;5;28mself\u001b[39m, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 21\u001b[0m     moves_proba_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayer\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m greedy:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Game(\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents,\n\u001b[1;32m     25\u001b[0m             random\u001b[38;5;241m.\u001b[39mchoices(\n\u001b[1;32m     26\u001b[0m                 \u001b[38;5;28mlist\u001b[39m(moves_proba_dict\u001b[38;5;241m.\u001b[39mkeys()), weights\u001b[38;5;241m=\u001b[39mmoves_proba_dict\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m     27\u001b[0m             )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msample_next_state(),\n\u001b[1;32m     28\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/agent.py:12\u001b[0m, in \u001b[0;36mAgent.play\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay\u001b[39m(\u001b[38;5;28mself\u001b[39m, state) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m---> 12\u001b[0m     moves_proba_dict, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_play_logic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Agent\u001b[38;5;241m.\u001b[39mfilter_moves(state, moves_proba_dict), value\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/MCTS_agent.py:82\u001b[0m, in \u001b[0;36mMCTSAgent._play_logic\u001b[0;34m(self, initial_state)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state\u001b[38;5;241m.\u001b[39mhas_ended:\n\u001b[1;32m     81\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_agent\u001b[38;5;241m.\u001b[39msample_move(state)\n\u001b[0;32m---> 82\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_next_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(state\u001b[38;5;241m.\u001b[39mreward[initial_state\u001b[38;5;241m.\u001b[39mplayer])\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "play_2players_games(MCTSAgent(3000), OneMoveAheadAgent(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0\n",
      "{'agent1': 0, 'agent2': 0, 'draws': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1\n",
      "{'agent1': 0, 'agent2': 1, 'draws': 0}\n",
      "Game 2\n",
      "{'agent1': 0, 'agent2': 2, 'draws': 0}\n",
      "Game 3\n",
      "{'agent1': 0, 'agent2': 3, 'draws': 0}\n",
      "Game 4\n",
      "{'agent1': 0, 'agent2': 4, 'draws': 0}\n",
      "Game 5\n",
      "{'agent1': 0, 'agent2': 5, 'draws': 0}\n",
      "Game 6\n",
      "{'agent1': 0, 'agent2': 6, 'draws': 0}\n",
      "Game 7\n",
      "{'agent1': 1, 'agent2': 6, 'draws': 0}\n",
      "Game 8\n",
      "{'agent1': 1, 'agent2': 7, 'draws': 0}\n",
      "Game 9\n",
      "{'agent1': 1, 'agent2': 8, 'draws': 0}\n",
      "Game 10\n",
      "{'agent1': 1, 'agent2': 9, 'draws': 0}\n",
      "Game 11\n",
      "{'agent1': 2, 'agent2': 9, 'draws': 0}\n",
      "Game 12\n",
      "{'agent1': 2, 'agent2': 10, 'draws': 0}\n",
      "Game 13\n",
      "{'agent1': 2, 'agent2': 11, 'draws': 0}\n"
     ]
    }
   ],
   "source": [
    "play_2players_games(MCTSAgent(3000), AlphaZeroAgent(neural_net_agent, 200), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': {'x': 3, 'y': 0}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 1, 'y': 0}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 3, 'y': 1}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 2, 'y': 0}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 3, 'y': 0}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 0, 'y': 1}, 'to': {'x': 0, 'y': 7}}, {'from': {'x': 2, 'y': 1}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 3, 'y': 0}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 1, 'y': 6}, 'to': {'x': 0, 'y': -1}}]\n",
      "[{'from': {'x': 3, 'y': 0}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 1, 'y': 5}, 'to': {'x': 0, 'y': -1}}, {'from': {'x': 2, 'y': 5}, 'to': {'x': 0, 'y': -1}}, {'from': {'x': 4, 'y': 5}, 'to': {'x': 0, 'y': -1}}, {'from': {'x': 5, 'y': 5}, 'to': {'x': 0, 'y': -1}}]\n",
      "[{'from': {'x': 0, 'y': 0}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 0, 'y': 2}, 'to': {'x': 0, 'y': 7}}, {'from': {'x': 3, 'y': 2}, 'to': {'x': 0, 'y': 7}}, {'from': {'x': 5, 'y': 2}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 2, 'y': 6}, 'to': {'x': 0, 'y': -1}}]\n",
      "[{'from': {'x': 1, 'y': 1}, 'to': {'x': 0, 'y': 7}}, {'from': {'x': 3, 'y': 1}, 'to': {'x': 0, 'y': 7}}]\n",
      "[{'from': {'x': 4, 'y': 1}, 'to': {'x': 0, 'y': 7}}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print(bounce_to_supervised_inputs(g.state))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m     \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#print(g.state.get_tensors())\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     g\u001b[38;5;241m=\u001b[39m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     23\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(g\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(lens\u001b[38;5;241m+\u001b[39m[\u001b[38;5;241m100\u001b[39m]):\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/game.py:21\u001b[0m, in \u001b[0;36mGame.play\u001b[0;34m(self, greedy)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay\u001b[39m(\u001b[38;5;28mself\u001b[39m, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 21\u001b[0m     moves_proba_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayer\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m greedy:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Game(\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents,\n\u001b[1;32m     25\u001b[0m             random\u001b[38;5;241m.\u001b[39mchoices(\n\u001b[1;32m     26\u001b[0m                 \u001b[38;5;28mlist\u001b[39m(moves_proba_dict\u001b[38;5;241m.\u001b[39mkeys()), weights\u001b[38;5;241m=\u001b[39mmoves_proba_dict\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m     27\u001b[0m             )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msample_next_state(),\n\u001b[1;32m     28\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/agent.py:12\u001b[0m, in \u001b[0;36mAgent.play\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay\u001b[39m(\u001b[38;5;28mself\u001b[39m, state) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m---> 12\u001b[0m     moves_proba_dict, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_play_logic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Agent\u001b[38;5;241m.\u001b[39mfilter_moves(state, moves_proba_dict), value\n",
      "File \u001b[0;32m~/Documents/alpha-connect/src/alpha_connect/one_move_ahead_agent.py:16\u001b[0m, in \u001b[0;36mOneMoveAheadAgent._play_logic\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action2 \u001b[38;5;129;01min\u001b[39;00m action\u001b[38;5;241m.\u001b[39msample_next_state()\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43maction2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_next_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhas_ended:\n\u001b[1;32m     17\u001b[0m             possible\u001b[38;5;241m.\u001b[39mremove(action)\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agents = [OneMoveAheadAgent(), RandomAgent()]  # random : mean: 37.813, max: 283, min: 4\n",
    "# selective/selective : mean: 20.0, max: 116, min: 7\n",
    "# selective/random : mean: 13.218, max: 71, min: 3\n",
    "lens = [-1]\n",
    "for j in range(500):\n",
    "    g = Game(agents)\n",
    "    i = 0\n",
    "    states = []\n",
    "    while not g.has_ended():\n",
    "        states.append(g.state)\n",
    "        i += 1\n",
    "        # print(\"-\"*50)\n",
    "        # print(g.state.player)\n",
    "        # print(g.state.get_tensors())\n",
    "        a = [a.to_json() for a in g.state.actions if a.to_json()[\"to\"][\"y\"] in [-1, 7]]\n",
    "        if len(a) != 0:\n",
    "            print(a)\n",
    "        # print(bounce_to_supervised_inputs(g.state))\n",
    "\n",
    "        # print(g.state.get_tensors())\n",
    "        g = g.play()\n",
    "    states.append(g.state)\n",
    "    if i <= min(lens + [100]):\n",
    "        print(\"new min\", i)\n",
    "        for state in states:\n",
    "            print(state.get_tensors())\n",
    "            print([a.to_json() for a in state.actions])\n",
    "    lens.append(i)\n",
    "\n",
    "print(f\"mean: {sum(lens)/len(lens)}, max: {max(lens)}, min: {min(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
